# oh-my-infra-learning

- The whole GitHub flow to build and push an image, allow it to connect to the DB on AWS, and deployment the application
  - ‚õ¥Ô∏è Build the image from the dockerfile in the directory

  _Important notes_
    - If docker-compose.yaml is used, it will not build an image that combines both database and the actual application (i.e.: SpringBoot/Go server application).Instead, the created container combines the 2 containers. Therefore, it's very handy to use that to fast test the applications. Also, the entrypoint in docker-compose will override that in Dockerfile.
        - Example:
          ``` 
          services:
            mysqlservice:
              image: mysql:8.0
              environment: 
                MYSQL_ROOT_PASSWORD: secret1
                MYSQL_DATABASE: fullstackdemo
            api:
              build:
                context: .
                dockerfile: Dockerfile
              ports:
                - "8080:8080"
              environment:
                - spring.datasource.url=jdbc:mysql://root:secret1@mysqlservice:3306/fullstackdemo
              depends_on:
                - mysqlservice
              entrypoint: ["java", "-jar", "app.jar"]
          ```
    - However, for the deployment purpose (on K8S), the image generated by Dockerfile (and pushed to docker registry should be used).
    - The container needs to connect to mysql service, which's also deployed on some pods.
        - Example:
          ```
          # Use an official OpenJDK runtime as a parent image
          FROM openjdk:25-jdk-slim
          
          # Set the working directory in the container
          WORKDIR /app
          
          # Copy the application's JAR file to the container
          COPY target/*.jar app.jar
          
          # Expose the port the app runs on
          EXPOSE 8080
          
          ENTRYPOINT [ "java", "-jar", "app.jar"]
          ```

- When using AWS ECR for the registry of image, the access key and access secrets on GitHub Action need to be an IAM user with specific permissions, which could be created on IAM console.
    - Permissions:
      ```
      AmazonEC2ContainerRegistryFullAccess
      eks:DescribeCluster
      SecretsManagerReadWrite (to retrieve password to the database)
      ```
- SecretsManager
  - General steps: 
    - Use Jq to get the KV pair of secrets
      ```
      aws secretsmanager get-secret-value --secret-id {{secret id on secret manager}}  --query SecretString --output text | jq -r  'to_entries|map("\(.key)=\(.value)")|.[]'
      ```
    - Create the elastic container registry on AWS to push the image
    - Log-in to ECR
      - Workflow example
      ```
       - name: Login to Amazon ECR
         id: login-ecr
         uses: aws-actions/amazon-ecr-login@v1
      ```
    - Push the image to the pre-created repo on the registry
      ```
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -t $ECR_REGISTRY/$ECR_REOSITORY:latest .
        docker push -a $ECR_REGISTRY/$ECR_REPOSITORY
      ```
- üìä What database is the container connecting with, and how to correctly config the database to enhance seamless operation of the application (CRUD)
  - Use the DB on AWS (RDS), 
    ```
    DB_Driver: pgsql
    DB_SOURCE=postgresql://root:secret1@{{endpoint on AWS RDS dashboard}} :5432/{{DB identifier on AWS}}
    ```
    As the GitHub workflow logins with a proper user (created by the root user on IAM), the GitHub Action uses a role that has permissions to read and write secrets, it can connect to the RDS DB.

  - or, use mysql deployment and service in the K8S context. _It must be in the same context, such as minikube, as the context where the application is deployed.
    - mysql-deployment.yaml
    ```
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: mysql
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: mysql
      template:
        metadata:
          labels:
            app: mysql
        spec:
          containers:
          - name: mysqlcontainer
            image: mysql:8
            env:
            - name: MYSQL_DATABASE
              value: (1) *must be the same as the database specified in the spring.datasource.url in the environment of the deployment of the application*
            - name: MYSQL_ROOT_PASSWORD
              value: secret1
            ports:
            - containerPort: 3306
    ```
    - mysql-service.yaml
    ```
    apiVersion: v1
    kind: Service
    metadata:
      name: (2) *the name, must be the same as the {{host}} specified in the spring.datasource.url in the environment of the deployment of the application*
    spec:
      selector:
        app: mysql
      ports:
        - protocol: TCP
          port: 3306
          targetPort: 3306
    ```
    _Explanation with example_
    Let's say,
      (1) _my-demo-db_
      (2) _my-demo-service_
    Then, the deployment.yaml needs to be configured like that
    ```
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: demo-deployment
      labels:
        app: my-demo1
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: my-demo1
      template:
        metadata:
          labels:
            app: my-demo1
        spec:
          containers:
          - name: onmyapi-container
            image: {{ image uri }}  
            imagePullPolicy: Always
            ports:
            - containerPort: 8080
            env: 
            - name: spring.datasource.url
              value: jdbc:mysql://root:secret1@my-demo-service:3306/my-demo-db
          imagePullSecrets:
            - name: demo-secret
    ```
- Deployment, K8S cluster, AWS EKS
  - If not deployment with EKS context, we can choose to use minikube
      - To view all configs, run `kubectl config get-contexts`
      - To switch between contexts, run `kubectl config use-context minikube`

  - After the context is switched, apply the yamls.
    - It requires secrets for private registry.
  
  - _Difference between service, deployment, ingress_

      - #### Deployment: the actual instance of the applicatio, managing a set of pods to run an application workload. 
          - The deployment can create/destory pods dynamatically.
          - Deployment enables that the container runs on pods, and that it listens on the -containerPort.

          - Service is needed to expose the service ports. 
            - Q: If some set of BE pods provides functionality to FE pods inside your cluster, how do the FEs find out and keep track of which IP address to connect to?
          Use service

      - #### Service: providing endpoints for accessing the application, regardless of the number of instances running (exposing necessary functionalities)
          - Publish the pods of demo-api that listen on TCP 8080 port
          - Incoming requests to port 80 of the service will be forwarded to port 8080
            - Type ClusterIP: 
            - Type NodePort: the control plane allocates a port from a range specified by --servide-node-port-range (nodePort is optional, because the control plane can allocate a default port)

      - #### Ingress: need to specify the port of the (different) services to make sure that requests are correctly forwarded.

- ü¶ú üêç First Python flask app with LangChain, chat models, LLM
  - Use GCP secret manager to load secrets
    - Step 1:
      - Create account on GCP, make sure to enable _Secret Manager API_ and _IAM Service Account Credentials API_. Create your project on GCP, and store the secrets (LangChain token, GitHub token and LLM model token) into [secret manager](https://console.cloud.google.com/security/secret-manager).
      - To enable the services, you have to enable a billing account. However, you will not be charged as long as your usage doesn't exceed the free tier limit.
    - Step 2:
      - On the VSCode terminal, login with gcloud
        ```bash
        gcloud auth application-default login
        ```
        This authenticates you use client libraries, so that you can have access to the credentials in your code.
        For example, `source_credentials, _ = google.auth.default()` in Python
        On the other hand, `gcloud auth login` will only allow you to use the gcloud CLI.
        Check the account you logged in with by

        ```bash
        gcloud auth list
        ```
        Config the project with

        ```bash
        gcloud iam servie-accounts create secret-accessor --description="Service account for accessing secrets" --display-name="Secret Accessor"
        ```
        The service account looks like `secret-accessor@{{project ID}}.iam.gserviceaccount.com`
    - Step 3:
      - Grant your user account impersonation access.
        ```bash
        gcloud iam service-accounts add-iam-policy-binding secret-accessor@hale-entry-460707-d0.iam.gserviceaccount.com \\n  --member="user:yuweichris8888@gmail.com" \\n  --role="roles/iam.serviceAccountTokenCreator"
        ```
      - üí£ Note: If an error like this occurrs:
        ```
        gcloud.iam.service-accounts.add-iam-policy-binding Failed to impersonate
        Make sure the account that's trying to impersonate it has access to the service account itself and the "roles/iam.serviceAccountTokenCreator" role.
        ```
        Diagnosis:
        You probably have executed this earlier
        ```
        gcloud config set auth/impersonate_service_account secret-accessor@{{project ID}}.iam.gserviceaccount.com
        ```
        
      - Test
    
